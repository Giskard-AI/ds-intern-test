{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Technical exercise - Data scientist intern @ Giskard\n",
    "\n",
    "Hi! As part of our recruitment process, we’d like you to complete the following technical test in 10 days. Once you finish the exercise, you can send your notebook or share your code repository by email (matteo@giskard.ai). If you want to share a private GitHub repository, make sure you give read access to `mattbit`.\n",
    "\n",
    "If you have problems running the notebook, get in touch with Matteo at matteo@giskard.ai."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install numpy pandas scikit-learn datasets transformers torch \"giskard>=2.0.0b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Code review\n",
    "\n",
    "Your fellow intern is working on securing our API and wrote some code to generate secure tokens. You have been asked to review their code and make sure it is secure and robust. Can you spot the problem and write a short feedback?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "ALPHABET = \"abcdefghijklmnopqrstuvxyz0123456789\"\n",
    "\n",
    "\n",
    "def generate_secret_key(size: int = 20):\n",
    "    \"\"\"Generates a cryptographically secure random token.\"\"\"\n",
    "    token = \"\".join(random.choice(ALPHABET) for _ in range(size))\n",
    "    return token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: High dimensions\n",
    "\n",
    "Matteo, our ML researcher, is struggling with a dataset of 40-dimensional points. He’s sure there are some clusters in there, but he does not know how many. Can you help him find the correct number of clusters in this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.load(\"points_1.npy\")\n",
    "\n",
    "# ...\n",
    "\n",
    "print(\"It looks like there are ??? clusters.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matteo is grateful for how you helped him with the cluster finding, and he has another problem for you. He has another high-dimensional dataset, but he thinks that those points could be represented in a lower dimensional space. Can you help him determine how many dimensions would be enough to well represent the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.load(\"points_2.npy\")\n",
    "\n",
    "# ...\n",
    "\n",
    "print(\"It looks the data is ???-dimensional\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Mad GPT\n",
    "\n",
    "Matteo is a good guy but he is a bit messy: he fine-tuned a GPT-2 model, but it seems that something went wrong during the process and the model became obsessed with early Romantic literature.\n",
    "\n",
    "Could you check how the model would continue a sentence starting with “Ty”? Could you recover the logit of the next best token? And its probability?\n",
    "\n",
    "You can get the model from the HuggingFace Hub as `mattbit/gpt2wb`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mattbit/gpt2wb\")\n",
    "\n",
    "# ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Not bad reviews\n",
    "\n",
    "\n",
    "We trained a random forest model to predict if a film review is positive or negative. Here is the training code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_data = datasets.load_dataset(\"sst2\", split=\"train[:20000]\").to_pandas()\n",
    "valid_data = datasets.load_dataset(\"sst2\", split=\"validation\").to_pandas()\n",
    "\n",
    "# Prepare model\n",
    "with open(\"stopwords.txt\", \"r\") as f:\n",
    "    stopwords = [w.strip() for w in f.readlines()]\n",
    "\n",
    "preprocessor = TfidfVectorizer(stop_words=stopwords, max_features=5000, lowercase=False)\n",
    "classifier = RandomForestClassifier(n_estimators=400, n_jobs=-1)\n",
    "\n",
    "model = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "\n",
    "# Train\n",
    "X = train_data.sentence\n",
    "y = train_data.label\n",
    "\n",
    "model.fit(X, y)\n",
    "\n",
    "print(\n",
    "    \"Training complete.\",\n",
    "    \"Accuracy:\",\n",
    "    model.score(valid_data.sentence, valid_data.label),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, it works quite well, but we noticed it has some problems with reviews containing negations, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class labels are:\n",
    "# 1 = Positive, 0 = Negative\n",
    "\n",
    "# this returns positive, that’s right!\n",
    "assert model.predict([\"This movie is good\"]) == [1]\n",
    "\n",
    "# negative! bingo!\n",
    "assert model.predict([\"This movie is bad\"]) == [0]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! uhm…\n",
    "assert model.predict([\"This movie is not bad at all!\"]) == [1]\n",
    "\n",
    "# WHOOPS! this ↓ is predicted as negative?! why?\n",
    "assert model.predict([\"This movie is not perfect, but very good!\"]) == [1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you help us understand what is going on? Do you have any idea on how to fix it?\n",
    "You can edit the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Model weaknesses\n",
    "\n",
    "\n",
    "The Giskard python library provides an automatic scanner to find weaknesses and vulnerabilities in ML models.\n",
    "\n",
    "Using this tool, could you identify some issues in the movie classification model above? Can you propose hypotheses about what is causing these issues?\n",
    "\n",
    "Then, choose one of the issues you just found and try to improve the model to mitigate or resolve it — just one, no need to spend the whole weekend over it!\n",
    "\n",
    "You can find a quickstart here: https://docs.giskard.ai/en/latest/getting-started/quickstart.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
